---
title: "HW 5"
author: "Kate Kong jk49633"
date: "2024-02-25"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

letter_frequencies <- read.csv('letter_frequencies.csv')

library(ggplot2)
library(dplyr)
suppressPackageStartupMessages(library(tidyverse))
library(mosaic)
library(stringr)
```

# Link to the GitHub
[GitHub](https://github.com/katekong27/SDS_315.git)


# **Question 1**
    
```{r echo=FALSE}
securities = do(10000)*nflip(n=2021, prob=0.024)
pval_trade = sum(securities >= 70) / 10000
ggplot(securities) + geom_histogram(aes(x=nflip), binwidth=2, col = 'black', fill = 'pink') + labs(title = "Sampling Distribution of Number of Flagged Securities Trades", x = "Number of Flagged Trades", y = "Frequency")
```

For this hypothesis test, the null hypothesis states that 2.4% of the Iron Bank securities trades are flagged just like any other trader. The test statistic is the number of Iron Bank securities trades that were flagged, which is 70 cases out of 2021 trades. There is a greater chance the null hypothesis will be false if the test statistic is greater. Using 10,000 monte carlo simulations, a sample of flagged trades was generated. The p-value for this simulation was `r pval_trade`. Because the p-value is so small (p < 0.01), the null hypothesis is rejected. Therefore, securities trade from Iron Bank was NOT flagged at the standard rate of 2.4% as proven by its small p-value. 


# **Question 2**

```{r echo=FALSE}
# 8 / 50 = 16%
inspections = do(10000)*nflip(n=50, prob=0.03)
pval_inspections = sum(inspections >= 8) / 10000
ggplot(inspections) + geom_histogram(aes(x=nflip), binwidth=0.5, col = 'skyblue', fill = 'skyblue') + labs(title = "Sampling Distribution of Number of Health Code Violations", x = "Number of Health Code Violations", y = "Frequency")
```

The null hypothesis for this hypothesis test would be, "Gourmet Bites was reported for violating the health code at a rate consistent with the city's average rate of 3%." In this case, the test statistic would be the number of times Gourmet Bites was flagged for health code violation, which is 8 out of 50 times. The greater the test statistic is, the more likely it is for the null hypothesis to turn out false. Based on the 10,000 Monte-Carlo simulations conducted above, the p-value came out to be `r pval_inspections`. Lower p-value indicate that there is less probability of the null hypothesis being true. In conclusion, since the p-value is zero, it can be said that the null hypothesis is rejected and the rate at which Gourmet Bites was caught violating the health code is NOT the same as the city's 3% baseline rate. 


# **Question 3**

## Part A

```{r echo=FALSE}
brown <- readLines("~/Desktop/SDS_315/brown_sentences.txt")
sentences = data.frame(values=brown)

x2 <- c()

for (i in 1:nrow(sentences)) {
  clean_sentence = gsub("[^A-Za-z]", "", sentences[i, 1])
  clean_sentence = toupper(clean_sentence)
  
  observed_freq = table(factor(strsplit(clean_sentence, "")[[1]], levels = letter_frequencies$Letter))
  observed <- as.numeric(observed_freq)

  expected = str_length(clean_sentence) * letter_frequencies$Probability
  
  result = sum((observed - expected)^2 / expected)
  x2 <- c(x2, result)
}

sentences$chi_square <- x2

ggplot(sentences) + geom_histogram(aes(x=chi_square), binwidth=5, col = 'navy', fill = 'skyblue') + labs(title = "Distribution of Chi-Square Values of Letter Frequencies of Brown Corpus Sentences", x = "Chi Square Values of Letter Frequencies", y = "Frequency")

```


## Part B

```{r echo=FALSE}
sentences_vector <- c(
  "She opened the book and started to read the first chapter, eagerly anticipating what might come next.",
  "Despite the heavy rain, they decided to go for a long walk in the park, crossing the main avenue by the fountain in the center.",
  "The museum’s new exhibit features ancient artifacts from various civilizations around the world.",
  "He carefully examined the document, looking for any clues that might help solve the mystery.",
  "The students gathered in the auditorium to listen to the guest speaker’s inspiring lecture.",
  "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland.",
  "The chef demonstrated how to prepare a delicious meal using only locally sourced ingredients, focusing mainly on some excellent dinner recipes from Spain.",
  "They watched the sunset from the hilltop, marveling at the beautiful array of colors in the sky.",
  "The committee reviewed the proposal and provided many points of useful feedback to improve the project’s effectiveness.",
  "Despite the challenges faced during the project, the team worked tirelessly to ensure its successful completion, resulting in a product that exceeded everyone’s expectations."
)

x2function <- function(n) {
  clean_sentence = gsub("[^A-Za-z]", "", sentences_vector[n])
  clean_sentence = toupper(clean_sentence)
  
  observed_freq = table(factor(strsplit(clean_sentence, "")[[1]], levels = letter_frequencies$Letter))
  observed <- as.numeric(observed_freq)

  expected = str_length(clean_sentence) * letter_frequencies$Probability
  
  result = sum((observed - expected)^2 / expected)
  return(result)
}

normal_english <- data.frame(value=sentences_vector)
for (i in 1:10) {
  normal_english$chi_square[i] <- x2function(i)
  normal_english$p_value[i] <-round(sum(sentences$chi_square >= normal_english$chi_square[i]) / nrow(sentences), 3)
}

normal_english[, 2:3]
```

The sentence generated by a large language model seems to be Sentence 6: "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland." This sentence has a noteably high chi-square value of `r as.numeric(normal_english[6, 2])`, indicating that it is far away from the sample distribution of chi-squares of letter frequencies seen in Brown Corpus Sentences. Similarly, its p-value is `r as.numeric(normal_english[6, 3])`, which is much lower than the p-values of other sentences. This shows that the letter frequencies of Sentence 6 is very different from the other nine sentences. Hence, Sentence 6 is the sentence generated by LLM.


